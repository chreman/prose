{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputpath anpassen\n",
    "datadir = \"/home/chris/data/dlina/prose/fontane_output_norm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawfiles = os.listdir(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Einlesen von Dateien im CoNLL tab-Format, wie es die Kallimachos-Engine ausgibt (https://gitlab2.informatik.uni-wuerzburg.de/kallimachos/KallimachosEngines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effi-briest\n",
      "Error tokenizing data. C error: EOF inside string starting at line 106696\n",
      "der-stechlin\n",
      "Error tokenizing data. C error: Expected 16 fields in line 28594, saw 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in rawfiles:\n",
    "    name = f.split(\"_\")[1]\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(datadir, f), sep=\"\\t\")\n",
    "        df.set_index(df.columns[:4].tolist(), inplace=True)\n",
    "    except Exception as e:\n",
    "        print(name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Begin</th>\n",
       "      <th>End</th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>CPOS</th>\n",
       "      <th>POS</th>\n",
       "      <th>Morphology</th>\n",
       "      <th>DependencyHead</th>\n",
       "      <th>DependencyRelation</th>\n",
       "      <th>NamedEntity</th>\n",
       "      <th>CorefId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SectionId</th>\n",
       "      <th>ParagraphId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>TokenId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>﻿An</td>\n",
       "      <td>&lt;unknown&gt;</td>\n",
       "      <td>N</td>\n",
       "      <td>NN</td>\n",
       "      <td>||</td>\n",
       "      <td>war</td>\n",
       "      <td>PD</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>einem</td>\n",
       "      <td>eine</td>\n",
       "      <td>PRO</td>\n",
       "      <td>ART</td>\n",
       "      <td>neut|sg|</td>\n",
       "      <td>﻿An</td>\n",
       "      <td>SB</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>der</td>\n",
       "      <td>die</td>\n",
       "      <td>ART</td>\n",
       "      <td>ART</td>\n",
       "      <td>neut|pl|</td>\n",
       "      <td>Maitage</td>\n",
       "      <td>NK</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>letzten</td>\n",
       "      <td>letzt</td>\n",
       "      <td>ADJA</td>\n",
       "      <td>ADJA</td>\n",
       "      <td>neut|pl|</td>\n",
       "      <td>Maitage</td>\n",
       "      <td>NK</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>Maitage</td>\n",
       "      <td>Maitag</td>\n",
       "      <td>N</td>\n",
       "      <td>NN</td>\n",
       "      <td>neut|pl|</td>\n",
       "      <td>einem</td>\n",
       "      <td>AG</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Begin      End      Token Lemma  \\\n",
       "SectionId ParagraphId SentenceId TokenId                                    \n",
       "0         0           0          0            3      ﻿An  <unknown>     N   \n",
       "                      1          4            9    einem       eine   PRO   \n",
       "                      2          10          13      der        die   ART   \n",
       "                      3          14          21  letzten      letzt  ADJA   \n",
       "                      4          22          29  Maitage     Maitag     N   \n",
       "\n",
       "                                          CPOS       POS Morphology  \\\n",
       "SectionId ParagraphId SentenceId TokenId                              \n",
       "0         0           0          0          NN        ||        war   \n",
       "                      1          4         ART  neut|sg|        ﻿An   \n",
       "                      2          10        ART  neut|pl|    Maitage   \n",
       "                      3          14       ADJA  neut|pl|    Maitage   \n",
       "                      4          22         NN  neut|pl|      einem   \n",
       "\n",
       "                                         DependencyHead DependencyRelation  \\\n",
       "SectionId ParagraphId SentenceId TokenId                                     \n",
       "0         0           0          0                   PD                  O   \n",
       "                      1          4                   SB                  O   \n",
       "                      2          10                  NK                  O   \n",
       "                      3          14                  NK                  O   \n",
       "                      4          22                  AG                  O   \n",
       "\n",
       "                                         NamedEntity  CorefId  \n",
       "SectionId ParagraphId SentenceId TokenId                       \n",
       "0         0           0          0                 -      NaN  \n",
       "                      1          4                 -      NaN  \n",
       "                      2          10                -      NaN  \n",
       "                      3          14                -      NaN  \n",
       "                      4          22                -      NaN  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NN', 'ART', 'ADJA', '$,', 'VAFIN', 'ADV', 'ADJD', 'VVFIN',\n",
       "       'APPRART', 'APZR', 'APPR', 'TRUNC', 'KON', 'PTKVZ', 'PAV', 'PPOSAT',\n",
       "       'CARD', 'PRELS', 'PIAT', 'VVPP', '$.', 'PRF', 'PTKZU', 'VVINF',\n",
       "       'PIS', 'KOUS', 'PDAT', 'PPER', 'KOKOM', 'VMFIN', 'PWAV', '$(', 'NE',\n",
       "       'VVIZU', 'PTKA', 'PTKNEG', 'VAINF', 'VAPP', 'ITJ', 'PDS', 'PTKANT',\n",
       "       'PWAT', 'PWS', 'KOUI', 'PRELAT', 'VVIMP', 'VAIMP', 'VMINF', 'APPO',\n",
       "       'FM', 'PPOSS'], dtype=object)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.CPOS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Ausgabe aller Named Entitys (aufsummiert nach Häufigkeit) im txt-Format. Diese Datei dient der manuelle Korrektur der Netzwerkdaten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird die entsprechende Funktion definiert die später auf jeden Roman angewendet wird. Übergeben wird der DataFrame sowie der Name des Romans, um das Ergebnis zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_entity_counts(df, name):\n",
    "    # first convert to Token column to a list and apply a Counter object to it\n",
    "    NE_counts = Counter(df[\"Token\"].tolist())\n",
    "    # next we count the entities, sort by most common appearance\n",
    "    # and transform it into a new dataframe with name and count columns\n",
    "    NE_counts = pd.DataFrame(NE_counts.most_common(), columns=[\"NE_name\", \"count\"])\n",
    "    # last, we save the counts to a csv\n",
    "    NE_counts.to_csv(os.path.join(\"results\", name+\"_entitycounts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Ausgabe von Relationslisten (d.i. Netzwerkdaten resp. Netzwerke) der Form „Akteur 1 – Akteur 2 – Häufigkeit“ als tsv-Datei, wobei für diese Relationslisten verschiedene Parametrisierungen vorgenommen werden können: \n",
    "* A) Akteure (also NEs) in einem Satz (abfragbar über sentenceID)\n",
    "* B) Akteure (also NEs) in einem Absatz (abfragbar über paragraphID)\n",
    "* C) Akteure (also NEs) in einem Absatz advanced, d.h. es werden nur Absätze mit mindestens einer bestimmten Anzahl von Wörtern (z.B. 500 oder 1000) berücksichtigt, wenn ein Absatz länger ist, wird der folgende dazugezählt. Die Anzahl der Wörter sollte einstellbar sein.\n",
    "* D) Akteure (also NEs) in einem Kapitel (abfragbar über sectionID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_network(df, name, aggregation):\n",
    "    \"\"\"Aggregations should be one of [\"SectionId, ParagraphId, SentenceId]\"\"\"\n",
    "    # first we group the tokens by the desired level of aggregation and count them\n",
    "    ne_occurrences = df.groupby(aggregation)[\"Token\"].count()\n",
    "    # next we filter out any tokens that only appear alone\n",
    "    # we then extract the indexes where entities co-occur\n",
    "    ne_cooc_inds = ne_occurrences[ne_occurrences > 1].index\n",
    "    # next follows a helper function that gives us the correct index to extract the entities\n",
    "    islice = get_index_slice(ne_cooc_inds, aggregation)\n",
    "    # next we group the co-occuring entities by the desired aggregation level\n",
    "    # and transform the grouped entities to lists\n",
    "    segments = (df.loc[islice, :][\"Token\"]\n",
    "                  .groupby(aggregation)\n",
    "                  .apply(lambda x: list(x)))\n",
    "    # in the next block we create the network\n",
    "    B = nx.Graph()\n",
    "    # we iterate over the segments (sections/paragraphs/sentences)\n",
    "    # items are co-occurring entities\n",
    "    for s, items in segments.iteritems():\n",
    "        source = str(s)\n",
    "        targets = items\n",
    "        if source not in B.nodes():\n",
    "            B.add_node(source, bipartite=0)\n",
    "        \n",
    "        for target in targets:\n",
    "            if target not in B.nodes():\n",
    "                B.add_node(target, bipartite=1)\n",
    "            B.add_edge(source, target)\n",
    "    \n",
    "    segment_nodes = set(n\n",
    "                        for n, d in B.nodes(data=True)\n",
    "                        if d['bipartite'] == 0)\n",
    "    entity_nodes = set(B) - segment_nodes\n",
    "    nx.is_bipartite(B)\n",
    "    # last, we project the bipartite graph of segments & entities into a an entitiy-cooccurrence graph\n",
    "    G = nx.bipartite.weighted_projected_graph(B, entity_nodes)\n",
    "    return G\n",
    "    \n",
    "def get_index_slice(ne_coocs_inds, aggregation):\n",
    "    if aggregation == \"SectionId\":\n",
    "        islice = pd.IndexSlice[ne_cooc_inds.tolist(), :, :]\n",
    "    if aggregation == \"ParagraphId\":\n",
    "        islice = pd.IndexSlice[:, ne_cooc_inds.tolist(), :]        \n",
    "    if aggregation == \"SentenceId\":\n",
    "        islice = pd.IndexSlice[:, :, ne_cooc_inds.tolist()]      \n",
    "    return islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Ausgabe von Netzwerkmetriken, wie sie z.B. vom Tool dramavis ausgegeben werden, für die unter 3) genannten Netzwerke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_character_metrics(G):\n",
    "    personae = G.nodes()\n",
    "    centralities = pd.DataFrame(index = [p for p in personae])\n",
    "    centralities.index.name = \"name\"\n",
    "    for metric in ['betweenness', 'degree',\n",
    "                   'closeness', 'closeness_corrected',\n",
    "                   'strength',\n",
    "                   'eigenvector_centrality']:\n",
    "        centralities[metric] = 0\n",
    "    for char, metric in nx.betweenness_centrality(G).items():\n",
    "        centralities.loc[char, 'betweenness'] = metric\n",
    "    for char, metric in nx.degree(G).items():\n",
    "        centralities.loc[char, 'degree'] = metric\n",
    "    for char, metric in nx.degree(G, weight=\"weight\").items():\n",
    "        centralities.loc[char, 'strength'] = metric\n",
    "    for char, metric in nx.closeness_centrality(G).items():\n",
    "        centralities.loc[char, 'closeness'] = metric\n",
    "    for g in nx.connected_component_subgraphs(G):\n",
    "        for char, metric in nx.closeness_centrality(g).items():\n",
    "            centralities.loc[char, 'closeness_corrected'] = metric\n",
    "    try:\n",
    "        for char, metric in nx.eigenvector_centrality(G,\n",
    "                                            max_iter=500).items():\n",
    "            centralities.loc[char, 'eigenvector_centrality'] = metric\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    centralities['avg_distance'] = 1/centralities['closeness']\n",
    "    centralities['avg_distance_corrected'] = 1/centralities['closeness_corrected']\n",
    "    return centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_metrics(G):\n",
    "    values = {}\n",
    "    values[\"charcount\"] = len(G.nodes())\n",
    "    values[\"edgecount\"] = len(G.edges())\n",
    "    try:\n",
    "        values[\"maxdegree\"] = max(G.degree().values())\n",
    "    except:\n",
    "        print(\"ValueError: max() arg is an empty sequence\")\n",
    "        values[\"maxdegree\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"avgdegree\"] = sum(G.degree().values())/len(G.nodes())\n",
    "    except:\n",
    "        values[\"avgdegree\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"density\"] = nx.density(G)\n",
    "    except:\n",
    "        values[\"density\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"avgpathlength\"] = nx.average_shortest_path_length(G)\n",
    "    except nx.NetworkXError:\n",
    "        try:\n",
    "            self.randomization = 50\n",
    "            values[\"avgpathlength\"] = nx.average_shortest_path_length(\n",
    "                        max(nx.connected_component_subgraphs(G), key=len))\n",
    "        except:\n",
    "            values[\"avgpathlength\"] = \"NaN\"\n",
    "    except:\n",
    "        values[\"avgpathlength\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"clustering_coefficient\"] = nx.average_clustering(G)\n",
    "    except:\n",
    "        values[\"clustering_coefficient\"] = \"NaN\"\n",
    "    values[\"connected_components\"] = nx.number_connected_components(G)\n",
    "    components = nx.connected_component_subgraphs(G)\n",
    "    values[\"component_sizes\"] = [len(c.nodes()) for c in components]\n",
    "    graph_metrics = pd.DataFrame.from_dict(values)\n",
    "    return graph_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGraph(G, figsize=(8, 8), filename=None):\n",
    "    \"\"\"\n",
    "    Plots an individual graph, node size by degree centrality,\n",
    "    edge size by edge weight.\n",
    "    \"\"\"\n",
    "    labels = {n:n for n in G.nodes()}\n",
    "\n",
    "    try:\n",
    "        # for networks with only one node\n",
    "        d = nx.degree_centrality(G)\n",
    "        nodesize = [v * 250 for v in d.values()]\n",
    "    except:\n",
    "        nodesize = [1 * 250 for n in G.nodes()]\n",
    "\n",
    "    layout=nx.spring_layout\n",
    "    pos=layout(G)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplots_adjust(left=0,right=1,bottom=0,top=0.95,wspace=0.01,hspace=0.01)\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G,pos,\n",
    "                            nodelist=G.nodes(),\n",
    "                            node_color=\"steelblue\",\n",
    "                            node_size=nodesize,\n",
    "                            alpha=0.8)\n",
    "    try:\n",
    "        weights = [G[u][v]['weight'] for u,v in G.edges()]\n",
    "    except:\n",
    "        weights = [1 for u,v in G.edges()]\n",
    "    nx.draw_networkx_edges(G,pos,\n",
    "                           with_labels=False,\n",
    "                           edge_color=\"grey\",\n",
    "                           width=weights\n",
    "                        )\n",
    "\n",
    "    if G.order() < 1000:\n",
    "        nx.draw_networkx_labels(G,pos, labels)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effi-briest: Error tokenizing data. C error: EOF inside string starting at line 106696\n",
      "der-stechlin: Error tokenizing data. C error: Expected 16 fields in line 28594, saw 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# set aggregation level here, SectionId, ParagraphId, SentenceId\n",
    "aggregation = \"SectionId\"\n",
    "######\n",
    "for f in rawfiles:\n",
    "    name = f.split(\"_\")[1]\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(datadir, f), sep=\"\\t\")\n",
    "        df.set_index(df.columns[:4].tolist(), inplace=True)\n",
    "        # we filter the token dataframe to include only tokens with CPOS==NE\n",
    "        df = df[df[\"CPOS\"] == \"NE\"]\n",
    "        # Anwendung von 2)\n",
    "        get_entity_counts(df, name)\n",
    "        # Anwendung von 3)\n",
    "        G = create_network(df, name, aggregation)\n",
    "        # Berechnung und Speicherung von 4)\n",
    "        char_metrics = get_character_metrics(G)\n",
    "        graph_metrics = get_graph_metrics(G)\n",
    "        char_metrics.to_csv(os.path.join(\"results\",\n",
    "                                         \"_\".join([name,\n",
    "                                                   aggregation,\n",
    "                                                   \"char_metrics.csv\"])))\n",
    "        graph_metrics.to_csv(os.path.join(\"results\",\n",
    "                                         \"_\".join([name,\n",
    "                                                   aggregation,\n",
    "                                                   \"graph_metrics.csv\"])))\n",
    "        plotGraph(G, filename=os.path.join(\"results\",\n",
    "                                           \"_\".join([name,\n",
    "                                                     aggregation+\".svg\"])))\n",
    "    except Exception as e:\n",
    "        print(\": \".join([name, str(e)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
