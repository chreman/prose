{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputpath anpassen\n",
    "datadir = \"/home/chris/data/dlina/prose/fontane_output_norm/\"\n",
    "metadatadir = \"/home/chris/data/dlina/prose/entitynames/\"\n",
    "excludedir = \"/home/chris/data/dlina/prose/exclude/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fontane_stine_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_effi-briest_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_ellernklipp_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_der-stechlin_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_ladultera_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_die-poggenpuhls_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_mathilde-moehring_ohne-paratexte_absaetze-normalisiert.txt.jtf',\n",
       " 'fontane_frau-jenny-treibl_ohne-paratexte_absaetze-normalisiert.txt.jtf']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles = os.listdir(datadir)\n",
    "rawfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Einlesen von Dateien im CoNLL tab-Format, wie es die Kallimachos-Engine ausgibt (https://gitlab2.informatik.uni-wuerzburg.de/kallimachos/KallimachosEngines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_raw(path):\n",
    "    with open(path, \"r\") as infile:\n",
    "        doc = infile.read()\n",
    "    splitlines = doc.split(\"\\n\")\n",
    "    header = splitlines[0].split(\"\\t\")\n",
    "    splitlines = [l.split(\"\\t\")[0:15] for l in splitlines]\n",
    "    splitlines = [l for l in splitlines if l != []]\n",
    "    df = pd.DataFrame(splitlines[1:])\n",
    "    df.columns = header\n",
    "    df.set_index(df.columns[:4].tolist(), inplace=True)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    df.index = df.index.droplevel(0)\n",
    "    df.sort_index(level=['ParagraphId','SentenceId', 'TokenId'], ascending=[1, 1, 1], inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_excludes(path):\n",
    "    pass\n",
    "\n",
    "def load_aliases(path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2) Ausgabe aller Named Entitys (aufsummiert nach Häufigkeit) im txt-Format. Diese Datei dient der manuelle Korrektur der Netzwerkdaten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hier wird die entsprechende Funktion definiert die später auf jeden Roman angewendet wird. Übergeben wird der DataFrame sowie der Name des Romans, um das Ergebnis zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_entity_counts(df, name):\n",
    "    # first convert to Token column to a list and apply a Counter object to it\n",
    "    NE_counts = Counter(df[\"Token\"].tolist())\n",
    "    # next we count the entities, sort by most common appearance\n",
    "    # and transform it into a new dataframe with name and count columns\n",
    "    NE_counts = pd.DataFrame(NE_counts.most_common(), columns=[\"NE_name\", \"count\"])\n",
    "    # last, we save the counts to a csv\n",
    "    NE_counts.to_csv(os.path.join(\"results\", name+\"_entitycounts.csv\"))\n",
    "    COREF_counts = df.groupby('CorefId')['Token'].apply(lambda x: Counter(x))\n",
    "    COREF_counts.to_csv(os.path.join(\"results\", \"_\".join([name, \"coref_entitycounts.csv\"])),\n",
    "                        index_label = [\"CorefId\", \"NE_name\"], header = [\"counts\"])\n",
    "    \n",
    "def get_entity_mapper(df):\n",
    "    mapper = df.groupby('CorefId')['Token'].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "    mapper = pd.DataFrame(mapper).reset_index()\n",
    "    mapper['nodename'] = mapper.apply(lambda x: \"-\".join([x['CorefId'], x['Token']]), axis=1)\n",
    "    mapper.set_index(\"CorefId\", inplace=True)\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Ausgabe von Relationsliste (d.i. Netzwerkdaten resp. Netzwerke) der Form „Akteur 1 – Akteur 2 – Häufigkeit“ als tsv-Datei, wobei für diese Relationslisten verschiedene Parametrisierungen vorgenommen werden können: \n",
    "* A) Akteure (also NEs) in einem Satz (abfragbar über sentenceID)\n",
    "* B) Akteure (also NEs) in einem Absatz (abfragbar über paragraphID)\n",
    "* C) Akteure (also NEs) in einem Absatz advanced, d.h. es werden nur Absätze mit mindestens einer bestimmten Anzahl von Wörtern (z.B. 500 oder 1000) berücksichtigt, wenn ein Absatz länger ist, wird der folgende dazugezählt. Die Anzahl der Wörter sollte einstellbar sein.\n",
    "* D) Akteure (also NEs) in einem Kapitel (abfragbar über sectionID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_network(df, name, aggregation, mapper, parasize, paralimit):\n",
    "    \"\"\"Aggregations should be one of [\"SectionId, ParagraphId, SentenceId]\"\"\"\n",
    "    if aggregation == \"ParagraphId\":\n",
    "        G = create_network_by_paragraph(df, name, aggregation, mapper, parasize, paralimit)\n",
    "    if aggregation == \"SentenceId\":\n",
    "        G = create_network_by_sentence(df, name, aggregation, mapper)\n",
    "    return G\n",
    "    \n",
    "    \n",
    "def create_network_by_paragraph(df, name, aggregation, mapper, parasize, paralimit):\n",
    "    # first we group the tokens by the desired level of aggregation and count them\n",
    "    ne_occurrences = df.groupby(aggregation)[\"CorefId\"].count()\n",
    "    # next we filter out any tokens that only appear alone\n",
    "    # we then extract the indexes where entities co-occur\n",
    "    ne_cooc_inds = ne_occurrences[ne_occurrences > 1].index.tolist()\n",
    "    # next follows a helper function that gives us the correct index to extract the entities\n",
    "    islice = get_index_slice(ne_cooc_inds, aggregation)\n",
    "    # next we group the co-occuring entities by the desired aggregation level\n",
    "    # and transform the grouped entities to lists\n",
    "    segments = (df.loc[islice, :][\"CorefId\"]\n",
    "                  .groupby(aggregation)\n",
    "                  .apply(lambda x: list(x)))\n",
    "    # in the next block we create the network\n",
    "    B = nx.Graph()\n",
    "    # we iterate over the segments (sections/paragraphs/sentences)\n",
    "    # items are co-occurring entities\n",
    "    cache = 0\n",
    "    cached_items = []\n",
    "    for s, items in segments.iteritems():\n",
    "        # source: segments\n",
    "        source = str(s)\n",
    "        if cache < paralimit:\n",
    "            cache += parasize.loc[s]\n",
    "            cached_items.extend(items)\n",
    "        else:\n",
    "            # targets: entities\n",
    "            targets = cached_items\n",
    "            if source not in B.nodes():\n",
    "                B.add_node(source, bipartite=0)\n",
    "\n",
    "            for target in targets:\n",
    "                if target not in B.nodes():\n",
    "                    B.add_node(target, bipartite=1)\n",
    "                B.add_edge(source, target)\n",
    "            cache = 0\n",
    "            cached_items = []\n",
    "    \n",
    "    segment_nodes = set(n\n",
    "                        for n, d in B.nodes(data=True)\n",
    "                        if d['bipartite'] == 0)\n",
    "    entity_nodes = set(B) - segment_nodes\n",
    "    nx.is_bipartite(B)\n",
    "    # last, we project the bipartite graph of segments & entities into a an entity-cooccurrence graph\n",
    "    G = nx.bipartite.weighted_projected_graph(B, entity_nodes)\n",
    "    return G\n",
    "    \n",
    "def create_network_by_sentence(df, name, aggregation, mapper):\n",
    "    # first we group the tokens by the desired level of aggregation and count them\n",
    "    ne_occurrences = df.groupby(aggregation)[\"CorefId\"].count()\n",
    "    # next we filter out any tokens that only appear alone\n",
    "    # we then extract the indexes where entities co-occur\n",
    "    ne_cooc_inds = ne_occurrences[ne_occurrences > 1].index.tolist()\n",
    "    # next follows a helper function that gives us the correct index to extract the entities\n",
    "    islice = get_index_slice(ne_cooc_inds, aggregation)\n",
    "    # next we group the co-occuring entities by the desired aggregation level\n",
    "    # and transform the grouped entities to lists\n",
    "    segments = (df.loc[islice, :][\"CorefId\"]\n",
    "                  .groupby(aggregation)\n",
    "                  .apply(lambda x: list(x)))\n",
    "    # in the next block we create the network\n",
    "    B = nx.Graph()\n",
    "    # we iterate over the segments (sections/paragraphs/sentences)\n",
    "    # items are co-occurring entities\n",
    "    for s, items in segments.iteritems():\n",
    "        source = str(s)\n",
    "        targets = items\n",
    "        if source not in B.nodes():\n",
    "            B.add_node(source, bipartite=0)\n",
    "        \n",
    "        for target in targets:\n",
    "            if target not in B.nodes():\n",
    "                B.add_node(target, bipartite=1)\n",
    "            B.add_edge(source, target)\n",
    "    \n",
    "    segment_nodes = set(n\n",
    "                        for n, d in B.nodes(data=True)\n",
    "                        if d['bipartite'] == 0)\n",
    "    entity_nodes = set(B) - segment_nodes\n",
    "    nx.is_bipartite(B)\n",
    "    # last, we project the bipartite graph of segments & entities into a an entity-cooccurrence graph\n",
    "    G = nx.bipartite.weighted_projected_graph(B, entity_nodes)\n",
    "    return G\n",
    "    \n",
    "def get_index_slice(ne_cooc_inds, aggregation):\n",
    "    if aggregation == \"SectionId\":\n",
    "        islice = pd.IndexSlice[ne_cooc_inds, :, :]\n",
    "    if aggregation == \"ParagraphId\":\n",
    "        islice = pd.IndexSlice[:, ne_cooc_inds, :]        \n",
    "    if aggregation == \"SentenceId\":\n",
    "        islice = pd.IndexSlice[:, :, ne_cooc_inds]      \n",
    "    return islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Ausgabe von Netzwerkmetriken, wie sie z.B. vom Tool dramavis ausgegeben werden, für die unter 3) genannten Netzwerke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_character_metrics(G):\n",
    "    personae = G.nodes()\n",
    "    centralities = pd.DataFrame(index = [p for p in personae])\n",
    "    centralities.index.name = \"name\"\n",
    "    for metric in ['betweenness', 'degree',\n",
    "                   'closeness', 'closeness_corrected',\n",
    "                   'strength',\n",
    "                   'eigenvector_centrality']:\n",
    "        centralities[metric] = 0\n",
    "    for char, metric in nx.betweenness_centrality(G).items():\n",
    "        centralities.loc[char, 'betweenness'] = metric\n",
    "    for char, metric in nx.degree(G).items():\n",
    "        centralities.loc[char, 'degree'] = metric\n",
    "    for char, metric in nx.degree(G, weight=\"weight\").items():\n",
    "        centralities.loc[char, 'strength'] = metric\n",
    "    for char, metric in nx.closeness_centrality(G).items():\n",
    "        centralities.loc[char, 'closeness'] = metric\n",
    "    for g in nx.connected_component_subgraphs(G):\n",
    "        for char, metric in nx.closeness_centrality(g).items():\n",
    "            centralities.loc[char, 'closeness_corrected'] = metric\n",
    "    try:\n",
    "        for char, metric in nx.eigenvector_centrality(G,\n",
    "                                            max_iter=500).items():\n",
    "            centralities.loc[char, 'eigenvector_centrality'] = metric\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    centralities['avg_distance'] = 1/centralities['closeness']\n",
    "    centralities['avg_distance_corrected'] = 1/centralities['closeness_corrected']\n",
    "    return centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_metrics(G):\n",
    "    values = {}\n",
    "    values[\"charcount\"] = len(G.nodes())\n",
    "    values[\"edgecount\"] = len(G.edges())\n",
    "    try:\n",
    "        values[\"maxdegree\"] = max(G.degree().values())\n",
    "    except:\n",
    "        print(\"ValueError: max() arg is an empty sequence\")\n",
    "        values[\"maxdegree\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"avgdegree\"] = sum(G.degree().values())/len(G.nodes())\n",
    "    except:\n",
    "        values[\"avgdegree\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"density\"] = nx.density(G)\n",
    "    except:\n",
    "        values[\"density\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"avgpathlength\"] = nx.average_shortest_path_length(G)\n",
    "    except nx.NetworkXError:\n",
    "        values[\"avgpathlength\"] = \"NaN\"\n",
    "\n",
    "    try:\n",
    "        values[\"clustering_coefficient\"] = nx.average_clustering(G)\n",
    "    except:\n",
    "        values[\"clustering_coefficient\"] = \"NaN\"\n",
    "    values[\"connected_components\"] = nx.number_connected_components(G)\n",
    "    components = nx.connected_component_subgraphs(G)\n",
    "    values[\"component_sizes\"] = [len(c.nodes()) for c in components]\n",
    "    graph_metrics = pd.DataFrame.from_dict(values)\n",
    "    return graph_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotGraph(G, mapper, figsize=(16, 16), filename=None):\n",
    "    \"\"\"\n",
    "    Plots an individual graph, node size by degree centrality,\n",
    "    edge size by edge weight.\n",
    "    \"\"\"\n",
    "    labels = mapper.to_dict()['nodename']\n",
    "\n",
    "    try:\n",
    "        # for networks with only one node\n",
    "        d = nx.degree_centrality(G)\n",
    "        nodesize = [v * 250 for v in d.values()]\n",
    "    except:\n",
    "        nodesize = [1 * 250 for n in G.nodes()]\n",
    "\n",
    "    layout=nx.spring_layout\n",
    "    pos=layout(G)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplots_adjust(left=0,right=1,bottom=0,top=0.95,wspace=0.01,hspace=0.01)\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G,pos,\n",
    "                            nodelist=G.nodes(),\n",
    "                            node_color=\"steelblue\",\n",
    "                            node_size=nodesize,\n",
    "                            alpha=0.8)\n",
    "    try:\n",
    "        weights = [G[u][v]['weight'] for u,v in G.edges()]\n",
    "    except:\n",
    "        weights = [1 for u,v in G.edges()]\n",
    "    nx.draw_networkx_edges(G,pos,\n",
    "                           with_labels=False,\n",
    "                           edge_color=\"grey\",\n",
    "                           width=weights\n",
    "                        )\n",
    "    labels = {k:v for k,v in labels.items() if k in G.nodes()}\n",
    "    if G.order() < 1000:\n",
    "        nx.draw_networkx_labels(G,pos, labels)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "# set aggregation level here, SectionId, ParagraphId, SentenceId\n",
    "# SectionId currently not implemented due to data not available\n",
    "aggregation = \"SentenceId\"\n",
    "paralimit = 1 # minimum of token count for paragraph concatenation\n",
    "selector = \"CorefId\" # either NamedEntity or CorefId\n",
    "######\n",
    "for f in rawfiles:\n",
    "    name = \"_\".join(f.split(\"_\")[:2])\n",
    "    try:\n",
    "        df = load_raw(os.path.join(datadir, f))\n",
    "    except Exception as e:\n",
    "        print(\": \".join([name, str(e)]))\n",
    "    parasize = df.groupby(\"ParagraphId\").count()[\"Token\"]\n",
    "    # we filter the token dataframe to include only named entity tokens\n",
    "    if selector == \"NamedEntity\":\n",
    "        df = df[df[\"NamedEntity\"].map(lambda x: x.endswith(\"PER_CORE\"))]\n",
    "    if selector == \"CorefId\":\n",
    "        df = df[df[\"CorefId\"].map(lambda x: x not in [\"-0\", \"-\"])]\n",
    "    # Anwendung von 2)\n",
    "    get_entity_counts(df, name)\n",
    "    mapper = get_entity_mapper(df)\n",
    "    # Anwendung von 3)\n",
    "    G = create_network(df, name, aggregation, mapper, parasize, paralimit=paralimit)\n",
    "    # export graph for re-use\n",
    "    nx.write_gml(G, os.path.join(\"results\", name+\".gml\"))\n",
    "    nx.write_edgelist(G, os.path.join(\"results\", name+\"_edgelist.csv\"),\n",
    "                      delimiter=\";\", data=[\"weight\"])\n",
    "    # Berechnung und Speicherung von 4)\n",
    "    char_metrics = get_character_metrics(G)\n",
    "    graph_metrics = get_graph_metrics(G)\n",
    "    char_metrics.to_csv(os.path.join(\"results\",\n",
    "                                     \"_\".join([name,\n",
    "                                               aggregation,\n",
    "                                               \"char_metrics.csv\"])))\n",
    "    graph_metrics.to_csv(os.path.join(\"results\",\n",
    "                                     \"_\".join([name,\n",
    "                                               aggregation,\n",
    "                                               \"graph_metrics.csv\"])))\n",
    "    plotGraph(G, mapper, filename=os.path.join(\"results\",\n",
    "                                       \"_\".join([name,\n",
    "                                                 aggregation+\".svg\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
